{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "eb031bbce033fa812dcec88de62f5abea6a352f76cdf43a5d1f21e2ea96289b4"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Project II. Local Feature Match\n",
    "\n",
    "1. Loads and resizes images  \n",
    "2. Finds interest points in those images                 (you code this)  \n",
    "3. Describes each interest point with a local feature    (you code this)  \n",
    "4. Finds matching features                               (you code this)  \n",
    "5. Visualizes the matches  \n",
    "6. Evaluates the matches based on ground truth correspondences  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import io, filters, feature, img_as_float32\n",
    "from skimage.transform import rescale\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import student\n",
    "import visualize\n",
    "from helpers import cheat_interest_points, evaluate_correspondence\n",
    "from utils import load_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should choose data_pair among [notre_dame, mt_rushmore, e_gaudi]\n",
    "data_pair = \"notre_dame\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Load in the data\n",
    "image1, image2, eval_file = load_data(data_pair)\n",
    "\n",
    "# You don't have to work with grayscale images. Matching with color\n",
    "# information might be helpful. If you choose to work with RGB images, just\n",
    "# comment these two lines and make sure scale_factor be changed, too.\n",
    "image1 = rgb2gray(image1)\n",
    "image2 = rgb2gray(image2)\n",
    "\n",
    "# make images smaller to speed up the algorithm. This parameter\n",
    "# gets passed into the evaluation code, so don't resize the images\n",
    "# except for changing this parameter - We will evaluate your code using\n",
    "# scale_factor = 0.5, so be aware of this\n",
    "\n",
    "scale_factor = 0.5\n",
    "# scale_factor = [0.5, 0.5, 1]\n",
    "\n",
    "# Bilinear rescaling\n",
    "image1 = np.float32(rescale(image1, scale_factor))\n",
    "image2 = np.float32(rescale(image2, scale_factor))\n",
    "# width and height of each local feature, in pixels\n",
    "feature_width = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Find distinctive points in each image. See Szeliski 4.1.1\n",
    "# !!! You will need to implement get_interest_points. !!!\n",
    "print(\"Getting interest points...\")\n",
    "\n",
    "# For development and debugging get_features and match_features, you will likely\n",
    "# want to use the ta ground truth points, you can comment out the precedeing two\n",
    "# lines and uncomment the following line to do this.\n",
    "\n",
    "#(x1, y1, x2, y2) = cheat_interest_points(eval_file, scale_factor)\n",
    "(x1, y1) = student.get_interest_points(image1, feature_width)\n",
    "(x2, y2) = student.get_interest_points(image2, feature_width)\n",
    "\n",
    "# if you want to view your corners uncomment these next lines!\n",
    "\n",
    "# plt.imshow(image1, cmap=\"gray\")\n",
    "# plt.scatter(x1, y1, alpha=0.9, s=3)\n",
    "# plt.show()\n",
    "# plt.imshow(image2, cmap=\"gray\")\n",
    "# plt.scatter(x2, y2, alpha=0.9, s=3)\n",
    "# plt.show()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Create feature vectors at each interest point. Szeliski 4.1.2\n",
    "# !!! You will need to implement get_features. !!!\n",
    "\n",
    "print(\"Getting features...\")\n",
    "image1_features = student.get_features(image1, x1, y1, feature_width)\n",
    "image2_features = student.get_features(image2, x2, y2, feature_width)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Match features. Szeliski 4.1.3\n",
    "# !!! You will need to implement match_features !!!\n",
    "\n",
    "print(\"Matching features...\")\n",
    "matches, confidences = student.match_features(image1_features, image2_features)\n",
    "\n",
    "if len(matches.shape) == 1:\n",
    "    print(\"No matches!\")\n",
    "    return\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Visualization\n",
    "# You might want to do some preprocessing of your interest points and matches\n",
    "# before visualizing (e.g. only visualizing 100 interest points). Once you\n",
    "# start detecting hundreds of interest points, the visualization can become\n",
    "# crowded. You may also want to threshold based on confidence\n",
    "# visualize.show_correspondences produces a figure that shows your matches\n",
    "# overlayed on the image pairs. evaluate_correspondence computes some statistics\n",
    "# about the quality of your matches, then shows the same figure. If you want to\n",
    "# just see the figure, you can uncomment the function call to visualize.show_correspondences\n",
    "\n",
    "num_pts_to_visualize = matches.shape[0]\n",
    "print(\"Matches: \" + str(num_pts_to_visualize))\n",
    "# visualize.show_correspondences(image1, image2, x1, y1, x2, y2, matches, filename=args.pair + \"_mtches.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6) Evaluation\n",
    "# This evaluation function will only work for the Notre Dame, Episcopal\n",
    "# Gaudi, and Mount Rushmore image pairs. Comment out this function if you\n",
    "# are not testing on those image pairs. Only those pairs have ground truth\n",
    "# available.\n",
    "#\n",
    "# It also only evaluates your top 100 matches by the confidences\n",
    "# that you provide.\n",
    "#\n",
    "# Within evaluate_correspondences(), we sort your matches in descending order\n",
    "#\n",
    "num_pts_to_evaluate = matches.shape[0]\n",
    "evaluate_correspondence(image1, image2, eval_file, scale_factor,\n",
    "    x1, y1, x2, y2, matches, confidences, num_pts_to_visualize)"
   ]
  }
 ]
}